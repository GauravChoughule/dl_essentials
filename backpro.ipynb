{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "758071d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/10000, Loss: 0.250677\n",
      "Epoch 200/10000, Loss: 0.250516\n",
      "Epoch 300/10000, Loss: 0.250377\n",
      "Epoch 400/10000, Loss: 0.250251\n",
      "Epoch 500/10000, Loss: 0.250135\n",
      "Epoch 600/10000, Loss: 0.250025\n",
      "Epoch 700/10000, Loss: 0.249918\n",
      "Epoch 800/10000, Loss: 0.249811\n",
      "Epoch 900/10000, Loss: 0.249701\n",
      "Epoch 1000/10000, Loss: 0.249587\n",
      "Epoch 1100/10000, Loss: 0.249464\n",
      "Epoch 1200/10000, Loss: 0.249329\n",
      "Epoch 1300/10000, Loss: 0.249180\n",
      "Epoch 1400/10000, Loss: 0.249011\n",
      "Epoch 1500/10000, Loss: 0.248817\n",
      "Epoch 1600/10000, Loss: 0.248592\n",
      "Epoch 1700/10000, Loss: 0.248327\n",
      "Epoch 1800/10000, Loss: 0.248013\n",
      "Epoch 1900/10000, Loss: 0.247634\n",
      "Epoch 2000/10000, Loss: 0.247176\n",
      "Epoch 2100/10000, Loss: 0.246616\n",
      "Epoch 2200/10000, Loss: 0.245928\n",
      "Epoch 2300/10000, Loss: 0.245079\n",
      "Epoch 2400/10000, Loss: 0.244031\n",
      "Epoch 2500/10000, Loss: 0.242740\n",
      "Epoch 2600/10000, Loss: 0.241158\n",
      "Epoch 2700/10000, Loss: 0.239238\n",
      "Epoch 2800/10000, Loss: 0.236942\n",
      "Epoch 2900/10000, Loss: 0.234248\n",
      "Epoch 3000/10000, Loss: 0.231161\n",
      "Epoch 3100/10000, Loss: 0.227720\n",
      "Epoch 3200/10000, Loss: 0.223999\n",
      "Epoch 3300/10000, Loss: 0.220100\n",
      "Epoch 3400/10000, Loss: 0.216136\n",
      "Epoch 3500/10000, Loss: 0.212214\n",
      "Epoch 3600/10000, Loss: 0.208421\n",
      "Epoch 3700/10000, Loss: 0.204810\n",
      "Epoch 3800/10000, Loss: 0.201407\n",
      "Epoch 3900/10000, Loss: 0.198206\n",
      "Epoch 4000/10000, Loss: 0.195179\n",
      "Epoch 4100/10000, Loss: 0.192279\n",
      "Epoch 4200/10000, Loss: 0.189443\n",
      "Epoch 4300/10000, Loss: 0.186594\n",
      "Epoch 4400/10000, Loss: 0.183635\n",
      "Epoch 4500/10000, Loss: 0.180453\n",
      "Epoch 4600/10000, Loss: 0.176911\n",
      "Epoch 4700/10000, Loss: 0.172851\n",
      "Epoch 4800/10000, Loss: 0.168085\n",
      "Epoch 4900/10000, Loss: 0.162398\n",
      "Epoch 5000/10000, Loss: 0.155542\n",
      "Epoch 5100/10000, Loss: 0.147263\n",
      "Epoch 5200/10000, Loss: 0.137389\n",
      "Epoch 5300/10000, Loss: 0.126006\n",
      "Epoch 5400/10000, Loss: 0.113599\n",
      "Epoch 5500/10000, Loss: 0.100971\n",
      "Epoch 5600/10000, Loss: 0.088951\n",
      "Epoch 5700/10000, Loss: 0.078107\n",
      "Epoch 5800/10000, Loss: 0.068677\n",
      "Epoch 5900/10000, Loss: 0.060643\n",
      "Epoch 6000/10000, Loss: 0.053861\n",
      "Epoch 6100/10000, Loss: 0.048146\n",
      "Epoch 6200/10000, Loss: 0.043316\n",
      "Epoch 6300/10000, Loss: 0.039214\n",
      "Epoch 6400/10000, Loss: 0.035708\n",
      "Epoch 6500/10000, Loss: 0.032692\n",
      "Epoch 6600/10000, Loss: 0.030080\n",
      "Epoch 6700/10000, Loss: 0.027804\n",
      "Epoch 6800/10000, Loss: 0.025807\n",
      "Epoch 6900/10000, Loss: 0.024045\n",
      "Epoch 7000/10000, Loss: 0.022483\n",
      "Epoch 7100/10000, Loss: 0.021089\n",
      "Epoch 7200/10000, Loss: 0.019841\n",
      "Epoch 7300/10000, Loss: 0.018718\n",
      "Epoch 7400/10000, Loss: 0.017703\n",
      "Epoch 7500/10000, Loss: 0.016782\n",
      "Epoch 7600/10000, Loss: 0.015944\n",
      "Epoch 7700/10000, Loss: 0.015178\n",
      "Epoch 7800/10000, Loss: 0.014475\n",
      "Epoch 7900/10000, Loss: 0.013830\n",
      "Epoch 8000/10000, Loss: 0.013235\n",
      "Epoch 8100/10000, Loss: 0.012684\n",
      "Epoch 8200/10000, Loss: 0.012174\n",
      "Epoch 8300/10000, Loss: 0.011701\n",
      "Epoch 8400/10000, Loss: 0.011260\n",
      "Epoch 8500/10000, Loss: 0.010848\n",
      "Epoch 8600/10000, Loss: 0.010464\n",
      "Epoch 8700/10000, Loss: 0.010103\n",
      "Epoch 8800/10000, Loss: 0.009765\n",
      "Epoch 8900/10000, Loss: 0.009447\n",
      "Epoch 9000/10000, Loss: 0.009148\n",
      "Epoch 9100/10000, Loss: 0.008866\n",
      "Epoch 9200/10000, Loss: 0.008600\n",
      "Epoch 9300/10000, Loss: 0.008348\n",
      "Epoch 9400/10000, Loss: 0.008109\n",
      "Epoch 9500/10000, Loss: 0.007883\n",
      "Epoch 9600/10000, Loss: 0.007669\n",
      "Epoch 9700/10000, Loss: 0.007465\n",
      "Epoch 9800/10000, Loss: 0.007271\n",
      "Epoch 9900/10000, Loss: 0.007086\n",
      "Epoch 10000/10000, Loss: 0.006910\n",
      "\n",
      "Predicted Output after training:\n",
      "[[0.08038901]\n",
      " [0.91264584]\n",
      " [0.90888899]\n",
      " [0.07236705]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of the sigmoid function\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Initialize the Neural Network\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1):\n",
    "        # Set learning rate\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Weights initialization (random between -1 and 1)\n",
    "        self.weights_input_hidden = np.random.uniform(-1, 1, (input_size, hidden_size))\n",
    "        self.weights_hidden_output = np.random.uniform(-1, 1, (hidden_size, output_size))\n",
    "        \n",
    "        # Biases initialization\n",
    "        self.bias_hidden = np.random.uniform(-1, 1, (1, hidden_size))\n",
    "        self.bias_output = np.random.uniform(-1, 1, (1, output_size))\n",
    "\n",
    "    # Forward pass function\n",
    "    def forward(self, X):\n",
    "        # Input to hidden layer\n",
    "        self.hidden_layer_activation = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
    "        self.hidden_layer_output = sigmoid(self.hidden_layer_activation)\n",
    "        \n",
    "        # Hidden layer to output layer\n",
    "        self.output_layer_activation = np.dot(self.hidden_layer_output, self.weights_hidden_output) + self.bias_output\n",
    "        self.predicted_output = sigmoid(self.output_layer_activation)\n",
    "        \n",
    "        return self.predicted_output\n",
    "\n",
    "    # Backpropagation function\n",
    "    def backward(self, X, y, predicted_output):\n",
    "        # Error in output\n",
    "        error = y - predicted_output\n",
    "        \n",
    "        # Gradient for output layer\n",
    "        d_output = error * sigmoid_derivative(predicted_output)\n",
    "        \n",
    "        # Error for hidden layer\n",
    "        error_hidden_layer = d_output.dot(self.weights_hidden_output.T)\n",
    "        \n",
    "        # Gradient for hidden layer\n",
    "        d_hidden_layer = error_hidden_layer * sigmoid_derivative(self.hidden_layer_output)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.weights_hidden_output += self.hidden_layer_output.T.dot(d_output) * self.learning_rate\n",
    "        self.bias_output += np.sum(d_output, axis=0, keepdims=True) * self.learning_rate\n",
    "        self.weights_input_hidden += X.T.dot(d_hidden_layer) * self.learning_rate\n",
    "        self.bias_hidden += np.sum(d_hidden_layer, axis=0, keepdims=True) * self.learning_rate\n",
    "\n",
    "    # Train the neural network\n",
    "    def train(self, X, y, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            predicted_output = self.forward(X)\n",
    "            self.backward(X, y, predicted_output)\n",
    "            \n",
    "            # Optionally, print the loss for every 100 epochs\n",
    "            if (epoch+1) % 100 == 0:\n",
    "                loss = np.mean(np.square(y - predicted_output))\n",
    "                print(f'Epoch {epoch+1}/{epochs}, Loss: {loss:.6f}')\n",
    "                \n",
    "# Generating dummy data\n",
    "np.random.seed(42)\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])  # Input: XOR problem\n",
    "y = np.array([[0], [1], [1], [0]])          # Output: XOR labels\n",
    "\n",
    "# Create and train the neural network\n",
    "nn = NeuralNetwork(input_size=2, hidden_size=2, output_size=1, learning_rate=0.1)\n",
    "nn.train(X, y, epochs=10000)\n",
    "\n",
    "# Test the network\n",
    "print(\"\\nPredicted Output after training:\")\n",
    "print(nn.forward(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea6fac7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
